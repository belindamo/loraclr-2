{"id":"loraclr_2025","title":"LoRACLR: Contrastive Adaptation for Customization of Diffusion Models","authors":"Enis Simsar, Thomas Hofmann, Federico Tombari, Pinar Yanardag","journal":"CVPR","year":"2025","doi":"10.48550/arXiv.2412.09622","url":"https://arxiv.org/abs/2412.09622","keyAssumptions":"Individual LoRA models work well for single concepts but fail when combined due to attention interference","keyHypotheses":"Contrastive objectives can align and merge weight spaces of multiple LoRA models while minimizing interference","strengths":"Training-free approach, addresses fundamental multi-concept generation problem, scalable","weaknesses":"Limited to LoRA-based approaches, evaluation primarily qualitative","citation":"Simsar et al. LoRACLR: Contrastive Adaptation for Customization of Diffusion Models. CVPR 2025.","notes":"Core paper establishing contrastive approach for LoRA merging","addedDate":"2025-08-28T16:10:00.000Z"}
{"id":"clora_2024","title":"Contrastive Test-Time Composition of Multiple LoRA Models for Image Generation","authors":"Tuna Han Salih Meral, Enis Simsar, Federico Tombari, Pinar Yanardag","journal":"CVPR","year":"2024","doi":"","url":"https://arxiv.org/html/2403.19776v2","keyAssumptions":"Attention maps within different LoRA models interfere causing concept omission or incorrect combinations","keyHypotheses":"Contrastive learning can separate attention maps for distinct concepts at test-time","strengths":"Training-free, uses attention maps for semantic masking, works with pre-trained LoRAs","weaknesses":"Computational overhead at inference time, limited theoretical analysis","citation":"Meral et al. Contrastive Test-Time Composition of Multiple LoRA Models. CVPR 2024.","notes":"Precursor work demonstrating contrastive approach for LoRA composition","addedDate":"2025-08-28T16:10:00.000Z"}
{"id":"dreambooth_2023","title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation","authors":"Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman","journal":"CVPR","year":"2023","doi":"10.48550/arXiv.2208.12242","url":"https://arxiv.org/abs/2208.12242","keyAssumptions":"Pre-trained models contain sufficient semantic priors; few images sufficient for subject binding","keyHypotheses":"Class-specific prior preservation prevents overfitting while enabling personalization","strengths":"High-quality results, established field, comprehensive evaluation","weaknesses":"Computationally expensive, requires full model fine-tuning, storage intensive","citation":"Ruiz et al. DreamBooth: Fine Tuning Text-to-Image Diffusion Models. CVPR 2023.","notes":"Foundational work for subject-driven generation, established the personalization paradigm","addedDate":"2025-08-28T16:10:00.000Z"}
{"id":"textual_inversion_2023","title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion","authors":"Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or","journal":"ICLR","year":"2023","doi":"10.48550/arXiv.2208.01618","url":"https://arxiv.org/abs/2208.01618","keyAssumptions":"Single pseudo-word embedding sufficient for capturing unique concepts; frozen models can learn new words","keyHypotheses":"Concepts can be represented through learned embeddings in text space","strengths":"Parameter-efficient, maintains model integrity, compositional","weaknesses":"Limited expressiveness, convergence challenges, concept-text alignment issues","citation":"Gal et al. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. ICLR 2023.","notes":"Established textual inversion paradigm for parameter-efficient personalization","addedDate":"2025-08-28T16:10:00.000Z"}
{"id":"conform_2024","title":"CONFORM: Contrast is All You Need For High-Fidelity Text-to-Image Diffusion Models","authors":"Tuna Han Salih Meral, Enis Simsar, Federico Tombari, Pınar Yanardağ","journal":"CVPR","year":"2024","doi":"","url":"https://arxiv.org/html/2312.06059v1","keyAssumptions":"Attention maps can be optimized using contrastive objectives; object segregation improves generation","keyHypotheses":"Contrastive learning can improve attention map quality for multi-object generation","strengths":"Training-free, works across models, versatile for multi-concept scenarios","weaknesses":"Requires test-time optimization, computational overhead","citation":"Meral et al. CONFORM: Contrast is All You Need For High-Fidelity Text-to-Image Diffusion Models. CVPR 2024.","notes":"Demonstrates contrastive learning for improving text-to-image generation quality","addedDate":"2025-08-28T16:10:00.000Z"}
{"id":"qr_lora_2025","title":"QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation","authors":"Various","journal":"ArXiv","year":"2025","doi":"","url":"https://arxiv.org/abs/2507.04599","keyAssumptions":"Standard LoRA leads to unstructured modifications causing feature entanglement","keyHypotheses":"QR decomposition provides structured parameter updates that separate visual features","strengths":"Reduces parameters by half, supports effective merging, strong disentanglement","weaknesses":"Limited to specific architectures, theoretical analysis incomplete","citation":"QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition. ArXiv 2025.","notes":"Novel LoRA variant using QR decomposition for better disentanglement","addedDate":"2025-08-28T16:10:00.000Z"}
{"id":"dreambooth_2023","title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation","authors":"Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman","journal":"CVPR","year":"2023","doi":"10.48550/arXiv.2208.12242","url":"https://arxiv.org/abs/2208.12242","keyAssumptions":"Pre-trained models contain sufficient semantic priors; few images sufficient for subject binding","keyHypotheses":"Class-specific prior preservation prevents overfitting while enabling personalization","strengths":"High-quality results, established field, comprehensive evaluation","weaknesses":"Computationally expensive, requires full model fine-tuning, storage intensive","citation":"Ruiz et al. DreamBooth: Fine Tuning Text-to-Image Diffusion Models. CVPR 2023.","notes":"Foundational work for subject-driven generation, established the personalization paradigm","addedDate":"2025-08-28T16:10:00.000Z"}
{"id":"dalle2_2022","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents","authors":"Aditya Ramesh et al.","journal":"ArXiv","year":"2022","doi":"","url":"https://arxiv.org/abs/2204.06125","keyAssumptions":"CLIP representations capture both semantics and style effectively","keyHypotheses":"Two-stage generation (prior + decoder) using CLIP embeddings improves diversity and control","strengths":"Established hierarchical generation paradigm, enables zero-shot manipulation","weaknesses":"Computational complexity of two-stage approach, requires CLIP alignment","citation":"Ramesh et al. Hierarchical Text-Conditional Image Generation with CLIP Latents. ArXiv 2022.","notes":"Foundational work establishing CLIP latents for hierarchical text-to-image generation","addedDate":"2025-08-28T16:10:00.000Z"}